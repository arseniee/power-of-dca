{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eecdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf593cb",
   "metadata": {},
   "source": [
    "# Generate a dataframe with websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8443d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the good links from main wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_hedge_funds\"\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract all <a> tags and collect their href attributes\n",
    "links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    # Wikipedia internal links often start with \"/wiki/\"\n",
    "    if href.startswith(\"/wiki/\") or href.startswith(\"http\"):\n",
    "        full_link = requests.compat.urljoin(url, href)\n",
    "        links.append(full_link)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "unique_links = list(dict.fromkeys(links))\n",
    "\n",
    "# Remopve all the useless links\n",
    "unique_links = unique_links[59:192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Website label found. https://en.wikipedia.org/wiki/Archegos_Capital_Management\n",
      "No Website label found. https://en.wikipedia.org/wiki/Baker_Brothers_Advisors\n",
      "No Website label found. https://en.wikipedia.org/wiki/D1_Capital_Partners\n",
      "No Website label found. https://en.wikipedia.org/wiki/Discovery_Capital_Management\n",
      "No Website label found. https://en.wikipedia.org/wiki/Ellington_Management_Group\n",
      "No Website label found. https://en.wikipedia.org/wiki/ESL_Investments\n",
      "No Website label found. https://en.wikipedia.org/wiki/Glenview_Capital_Management\n",
      "No Website label found. https://en.wikipedia.org/wiki/GoldenTree_Asset_Management\n",
      "No Website label found. https://en.wikipedia.org/wiki/Paulson_%26_Co.\n",
      "No Website label found. https://en.wikipedia.org/wiki/Quantum_Fund\n",
      "No Website label found. https://en.wikipedia.org/wiki/Touradji_Capital_Management\n",
      "128\n",
      "No link found in Website field. https://en.wikipedia.org/wiki/Rokos_Capital_Management\n",
      "No Website label found. https://en.wikipedia.org/wiki/Scipion_Capital\n"
     ]
    }
   ],
   "source": [
    "# Get the links to every website\n",
    "\n",
    "def get_website(urls):\n",
    "    res = []\n",
    "\n",
    "    for k, url in enumerate(urls):\n",
    "        # Fetch the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the <th> with the text 'Website'\n",
    "        website_th = soup.find(\"th\", class_=\"infobox-label\", string=\"Website\")\n",
    "\n",
    "        if website_th:\n",
    "            # Get the next <td> after this <th>\n",
    "            website_td = website_th.find_next_sibling(\"td\")\n",
    "            if website_td:\n",
    "                a_tag = website_td.find(\"a\", href=True)\n",
    "                if a_tag:\n",
    "                    website_link = a_tag[\"href\"]\n",
    "                    res.append(website_link)\n",
    "                else:\n",
    "                    res.append(False)\n",
    "                    print(k)\n",
    "                    print(\"No link found in Website field.\", url)\n",
    "            else:\n",
    "                res.append(False)\n",
    "                print(\"No <td> found after Website label.\", url)\n",
    "        else:\n",
    "            res.append(False)\n",
    "            print(\"No Website label found.\", url)\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "res = get_website(unique_links)\n",
    "res[128] = 'https://www.rokoscapital.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680517e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and save the final dataframe\n",
    "\n",
    "df = pd.DataFrame([], columns = ['wiki', 'website'])\n",
    "df['wiki'] = unique_links\n",
    "df['website'] = res\n",
    "df['company'] = df['wiki'].str.split('/wiki/').apply(lambda x: x[-1])\n",
    "df.to_csv('company_websites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83193f",
   "metadata": {},
   "source": [
    "# Extract the data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233ccf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>wiki</th>\n",
       "      <th>website</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Acadian_Asset_Ma...</td>\n",
       "      <td>http://www.acadian-asset.com</td>\n",
       "      <td>Acadian_Asset_Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adage_Capital_Ma...</td>\n",
       "      <td>http://www.adagecapital.com</td>\n",
       "      <td>Adage_Capital_Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alphadyne_Asset_...</td>\n",
       "      <td>http://adyne.com</td>\n",
       "      <td>Alphadyne_Asset_Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://en.wikipedia.org/wiki/AlphaSimplex_Group</td>\n",
       "      <td>http://www.alphasimplex.com</td>\n",
       "      <td>AlphaSimplex_Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Altimeter_Capital</td>\n",
       "      <td>http://altimeter.com</td>\n",
       "      <td>Altimeter_Capital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               wiki  \\\n",
       "0           0  https://en.wikipedia.org/wiki/Acadian_Asset_Ma...   \n",
       "1           1  https://en.wikipedia.org/wiki/Adage_Capital_Ma...   \n",
       "2           2  https://en.wikipedia.org/wiki/Alphadyne_Asset_...   \n",
       "3           3   https://en.wikipedia.org/wiki/AlphaSimplex_Group   \n",
       "4           4    https://en.wikipedia.org/wiki/Altimeter_Capital   \n",
       "\n",
       "                        website                     company  \n",
       "0  http://www.acadian-asset.com    Acadian_Asset_Management  \n",
       "1   http://www.adagecapital.com    Adage_Capital_Management  \n",
       "2              http://adyne.com  Alphadyne_Asset_Management  \n",
       "3   http://www.alphasimplex.com          AlphaSimplex_Group  \n",
       "4          http://altimeter.com           Altimeter_Capital  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('company_websites.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "152e685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_page(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_emails_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    emails = set()\n",
    "\n",
    "    # Look for mailto links\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith('mailto:'):\n",
    "            email = href[7:]  # Remove 'mailto:'\n",
    "            emails.add(email)\n",
    "\n",
    "    # Also look in the plain text\n",
    "    text_content = soup.get_text()\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails.update(re.findall(email_pattern, text_content, re.IGNORECASE))\n",
    "\n",
    "    return list(emails)\n",
    "\n",
    "def get_emails_from_url(url):\n",
    "    html_content = fetch_page(url)\n",
    "    if html_content is None:\n",
    "        return []\n",
    "    emails = extract_emails_from_html(html_content)\n",
    "    return list(set(emails))  # Remove duplicates\n",
    "\n",
    "def get_emails_from_urls(urls):\n",
    "    email_dict = {}\n",
    "    for url in urls:\n",
    "        print(f\"Processing: {url}\")\n",
    "        emails = get_emails_from_url(url)\n",
    "        email_dict[url] = emails\n",
    "    return email_dict\n",
    "\n",
    "def read_urls_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        urls = [line.strip() for line in file if line.strip()]\n",
    "    return urls\n",
    "\n",
    "def write_emails_to_file(email_dict, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for url, emails in email_dict.items():\n",
    "            file.write(f\"URL: {url}\\n\")\n",
    "            file.write(\"Emails found: \" + \", \".join(emails) + \"\\n\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage with file input/output\n",
    "#     input_file = \"urls.txt\"  # File containing URLs, one per line\n",
    "#     output_file = \"emails.txt\"  # File to save the results\n",
    "\n",
    "#     urls = read_urls_from_file(input_file)\n",
    "#     emails = get_emails_from_urls(urls)\n",
    "#     write_emails_to_file(emails, output_file)\n",
    "#     print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d5aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: http://www.acadian-asset.com\n",
      "Processing: http://www.adagecapital.com\n",
      "Processing: http://adyne.com\n",
      "Processing: http://www.alphasimplex.com\n",
      "Processing: http://altimeter.com\n",
      "Processing: http://angelogordon.com\n",
      "Error fetching http://angelogordon.com: 403 Client Error: Forbidden for url: http://angelogordon.com/\n",
      "Processing: http://amlp.com\n",
      "Error fetching http://amlp.com: HTTPConnectionPool(host='amlp.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001E2DE3FB020>: Failed to resolve 'amlp.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Processing: http://aqr.com\n",
      "Processing: False\n",
      "Error fetching False: Invalid URL 'False': No scheme supplied. Perhaps you meant https://False?\n",
      "Processing: https://web.archive.org/web/20230619183139/https://www.assuredinvestmentmanagement.com/\n"
     ]
    }
   ],
   "source": [
    "emails = get_emails_from_urls(df['website'].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6a6847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://www.acadian-asset.com': [],\n",
       " 'http://www.adagecapital.com': [],\n",
       " 'http://adyne.com': ['LegalCompliance@adyne.com'],\n",
       " 'http://www.alphasimplex.com': [],\n",
       " 'http://altimeter.com': ['IR@altimeter.com',\n",
       "  'press@altimeter.com',\n",
       "  'info@altimetercapital.com'],\n",
       " 'http://angelogordon.com': [],\n",
       " 'http://amlp.com': [],\n",
       " 'http://aqr.com': [],\n",
       " 'False': [],\n",
       " 'https://web.archive.org/web/20230619183139/https://www.assuredinvestmentmanagement.com/': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
